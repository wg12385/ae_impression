<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.4" />
<title>autoENRICH.ml.models.BCAI.modules.radam API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>autoENRICH.ml.models.BCAI.modules.radam</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">## This source code is from RAdam written by Liyuan Liu
##   (https://github.com/LiyuanLucasLiu/RAdam/tree/master)
##
##
##                                 Apache License
##                           Version 2.0, January 2004
##                        http://www.apache.org/licenses/
##
##   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
##
##   1. Definitions.
##
##      &#34;License&#34; shall mean the terms and conditions for use, reproduction,
##      and distribution as defined by Sections 1 through 9 of this document.
##
##      &#34;Licensor&#34; shall mean the copyright owner or entity authorized by
##      the copyright owner that is granting the License.
##
##      &#34;Legal Entity&#34; shall mean the union of the acting entity and all
##      other entities that control, are controlled by, or are under common
##      control with that entity. For the purposes of this definition,
##      &#34;control&#34; means (i) the power, direct or indirect, to cause the
##      direction or management of such entity, whether by contract or
##      otherwise, or (ii) ownership of fifty percent (50%) or more of the
##      outstanding shares, or (iii) beneficial ownership of such entity.
##
##      &#34;You&#34; (or &#34;Your&#34;) shall mean an individual or Legal Entity
##      exercising permissions granted by this License.
##
##      &#34;Source&#34; form shall mean the preferred form for making modifications,
##      including but not limited to software source code, documentation
##      source, and configuration files.
##
##      &#34;Object&#34; form shall mean any form resulting from mechanical
##      transformation or translation of a Source form, including but
##      not limited to compiled object code, generated documentation,
##      and conversions to other media types.
##
##      &#34;Work&#34; shall mean the work of authorship, whether in Source or
##      Object form, made available under the License, as indicated by a
##      copyright notice that is included in or attached to the work
##      (an example is provided in the Appendix below).
##
##      &#34;Derivative Works&#34; shall mean any work, whether in Source or Object
##      form, that is based on (or derived from) the Work and for which the
##      editorial revisions, annotations, elaborations, or other modifications
##      represent, as a whole, an original work of authorship. For the purposes
##      of this License, Derivative Works shall not include works that remain
##      separable from, or merely link (or bind by name) to the interfaces of,
##      the Work and Derivative Works thereof.
##
##      &#34;Contribution&#34; shall mean any work of authorship, including
##      the original version of the Work and any modifications or additions
##      to that Work or Derivative Works thereof, that is intentionally
##      submitted to Licensor for inclusion in the Work by the copyright owner
##      or by an individual or Legal Entity authorized to submit on behalf of
##      the copyright owner. For the purposes of this definition, &#34;submitted&#34;
##      means any form of electronic, verbal, or written communication sent
##      to the Licensor or its representatives, including but not limited to
##      communication on electronic mailing lists, source code control systems,
##      and issue tracking systems that are managed by, or on behalf of, the
##      Licensor for the purpose of discussing and improving the Work, but
##      excluding communication that is conspicuously marked or otherwise
##      designated in writing by the copyright owner as &#34;Not a Contribution.&#34;
##
##      &#34;Contributor&#34; shall mean Licensor and any individual or Legal Entity
##      on behalf of whom a Contribution has been received by Licensor and
##      subsequently incorporated within the Work.
##
##   2. Grant of Copyright License. Subject to the terms and conditions of
##      this License, each Contributor hereby grants to You a perpetual,
##      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
##      copyright license to reproduce, prepare Derivative Works of,
##      publicly display, publicly perform, sublicense, and distribute the
##      Work and such Derivative Works in Source or Object form.
##
##   3. Grant of Patent License. Subject to the terms and conditions of
##      this License, each Contributor hereby grants to You a perpetual,
##      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
##      (except as stated in this section) patent license to make, have made,
##      use, offer to sell, sell, import, and otherwise transfer the Work,
##      where such license applies only to those patent claims licensable
##      by such Contributor that are necessarily infringed by their
##      Contribution(s) alone or by combination of their Contribution(s)
##      with the Work to which such Contribution(s) was submitted. If You
##      institute patent litigation against any entity (including a
##      cross-claim or counterclaim in a lawsuit) alleging that the Work
##      or a Contribution incorporated within the Work constitutes direct
##      or contributory patent infringement, then any patent licenses
##      granted to You under this License for that Work shall terminate
##      as of the date such litigation is filed.
##
##   4. Redistribution. You may reproduce and distribute copies of the
##      Work or Derivative Works thereof in any medium, with or without
##      modifications, and in Source or Object form, provided that You
##      meet the following conditions:
##
##      (a) You must give any other recipients of the Work or
##          Derivative Works a copy of this License; and
##
##      (b) You must cause any modified files to carry prominent notices
##          stating that You changed the files; and
##
##      (c) You must retain, in the Source form of any Derivative Works
##          that You distribute, all copyright, patent, trademark, and
##          attribution notices from the Source form of the Work,
##          excluding those notices that do not pertain to any part of
##          the Derivative Works; and
##
##      (d) If the Work includes a &#34;NOTICE&#34; text file as part of its
##          distribution, then any Derivative Works that You distribute must
##          include a readable copy of the attribution notices contained
##          within such NOTICE file, excluding those notices that do not
##          pertain to any part of the Derivative Works, in at least one
##          of the following places: within a NOTICE text file distributed
##          as part of the Derivative Works; within the Source form or
##          documentation, if provided along with the Derivative Works; or,
##          within a display generated by the Derivative Works, if and
##          wherever such third-party notices normally appear. The contents
##          of the NOTICE file are for informational purposes only and
##          do not modify the License. You may add Your own attribution
##          notices within Derivative Works that You distribute, alongside
##          or as an addendum to the NOTICE text from the Work, provided
##          that such additional attribution notices cannot be construed
##          as modifying the License.
##
##      You may add Your own copyright statement to Your modifications and
##      may provide additional or different license terms and conditions
##      for use, reproduction, or distribution of Your modifications, or
##      for any such Derivative Works as a whole, provided Your use,
##      reproduction, and distribution of the Work otherwise complies with
##      the conditions stated in this License.
##
##   5. Submission of Contributions. Unless You explicitly state otherwise,
##      any Contribution intentionally submitted for inclusion in the Work
##      by You to the Licensor shall be under the terms and conditions of
##      this License, without any additional terms or conditions.
##      Notwithstanding the above, nothing herein shall supersede or modify
##      the terms of any separate license agreement you may have executed
##      with Licensor regarding such Contributions.
##
##   6. Trademarks. This License does not grant permission to use the trade
##      names, trademarks, service marks, or product names of the Licensor,
##      except as required for reasonable and customary use in describing the
##      origin of the Work and reproducing the content of the NOTICE file.
##
##   7. Disclaimer of Warranty. Unless required by applicable law or
##      agreed to in writing, Licensor provides the Work (and each
##      Contributor provides its Contributions) on an &#34;AS IS&#34; BASIS,
##      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
##      implied, including, without limitation, any warranties or conditions
##      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
##      PARTICULAR PURPOSE. You are solely responsible for determining the
##      appropriateness of using or redistributing the Work and assume any
##      risks associated with Your exercise of permissions under this License.
##
##   8. Limitation of Liability. In no event and under no legal theory,
##      whether in tort (including negligence), contract, or otherwise,
##      unless required by applicable law (such as deliberate and grossly
##      negligent acts) or agreed to in writing, shall any Contributor be
##      liable to You for damages, including any direct, indirect, special,
##      incidental, or consequential damages of any character arising as a
##      result of this License or out of the use or inability to use the
##      Work (including but not limited to damages for loss of goodwill,
##      work stoppage, computer failure or malfunction, or any and all
##      other commercial damages or losses), even if such Contributor
##      has been advised of the possibility of such damages.
##
##   9. Accepting Warranty or Additional Liability. While redistributing
##      the Work or Derivative Works thereof, You may choose to offer,
##      and charge a fee for, acceptance of support, warranty, indemnity,
##      or other liability obligations and/or rights consistent with this
##      License. However, in accepting such obligations, You may act only
##      on Your own behalf and on Your sole responsibility, not on behalf
##      of any other Contributor, and only if You agree to indemnify,
##      defend, and hold each Contributor harmless for any liability
##      incurred by, or claims asserted against, such Contributor by reason
##      of your accepting any such warranty or additional liability.
##
##   END OF TERMS AND CONDITIONS
##
##   APPENDIX: How to apply the Apache License to your work.
##
##      To apply the Apache License to your work, attach the following
##      boilerplate notice, with the fields enclosed by brackets &#34;[]&#34;
##      replaced with your own identifying information. (Don&#39;t include
##      the brackets!)  The text should be enclosed in the appropriate
##      comment syntax for the file format. We also recommend that a
##      file or class name and description of purpose be included on the
##      same &#34;printed page&#34; as the copyright notice for easier
##      identification within third-party archives.
##
##   Copyright [2019] [Liyuan Liu]
##
##   Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
##   you may not use this file except in compliance with the License.
##   You may obtain a copy of the License at
##
##       http://www.apache.org/licenses/LICENSE-2.0
##
##   Unless required by applicable law or agreed to in writing, software
##   distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
##   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
##   See the License for the specific language governing permissions and
##   limitations under the License.

import math
import torch
from torch.optim.optimizer import Optimizer, required

class RAdam(Optimizer):

    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        self.buffer = [[None, None, None] for ind in range(10)]
        super(RAdam, self).__init__(params, defaults)

    def __setstate__(self, state):
        super(RAdam, self).__setstate__(state)

    def step(self, closure=None):

        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:

            for p in group[&#39;params&#39;]:
                if p.grad is None:
                    continue
                grad = p.grad.data.float()
                if grad.is_sparse:
                    raise RuntimeError(&#39;RAdam does not support sparse gradients&#39;)

                p_data_fp32 = p.data.float()

                state = self.state[p]

                if len(state) == 0:
                    state[&#39;step&#39;] = 0
                    state[&#39;exp_avg&#39;] = torch.zeros_like(p_data_fp32)
                    state[&#39;exp_avg_sq&#39;] = torch.zeros_like(p_data_fp32)
                else:
                    state[&#39;exp_avg&#39;] = state[&#39;exp_avg&#39;].type_as(p_data_fp32)
                    state[&#39;exp_avg_sq&#39;] = state[&#39;exp_avg_sq&#39;].type_as(p_data_fp32)

                exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;]
                beta1, beta2 = group[&#39;betas&#39;]

                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
                exp_avg.mul_(beta1).add_(1 - beta1, grad)

                state[&#39;step&#39;] += 1
                buffered = self.buffer[int(state[&#39;step&#39;] % 10)]
                if state[&#39;step&#39;] == buffered[0]:
                    N_sma, step_size = buffered[1], buffered[2]
                else:
                    buffered[0] = state[&#39;step&#39;]
                    beta2_t = beta2 ** state[&#39;step&#39;]
                    N_sma_max = 2 / (1 - beta2) - 1
                    N_sma = N_sma_max - 2 * state[&#39;step&#39;] * beta2_t / (1 - beta2_t)
                    buffered[1] = N_sma

                    # more conservative since it&#39;s an approximated value
                    if N_sma &gt;= 5:
                        step_size = group[&#39;lr&#39;] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state[&#39;step&#39;])
                    else:
                        step_size = group[&#39;lr&#39;] / (1 - beta1 ** state[&#39;step&#39;])
                    buffered[2] = step_size

                if group[&#39;weight_decay&#39;] != 0:
                    p_data_fp32.add_(-group[&#39;weight_decay&#39;] * group[&#39;lr&#39;], p_data_fp32)

                # more conservative since it&#39;s an approximated value
                if N_sma &gt;= 5:            
                    denom = exp_avg_sq.sqrt().add_(group[&#39;eps&#39;])
                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)
                else:
                    p_data_fp32.add_(-step_size, exp_avg)

                p.data.copy_(p_data_fp32)

        return loss

class PlainRAdam(Optimizer):

    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)

        super(PlainRAdam, self).__init__(params, defaults)

    def __setstate__(self, state):
        super(PlainRAdam, self).__setstate__(state)

    def step(self, closure=None):

        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:

            for p in group[&#39;params&#39;]:
                if p.grad is None:
                    continue
                grad = p.grad.data.float()
                if grad.is_sparse:
                    raise RuntimeError(&#39;RAdam does not support sparse gradients&#39;)

                p_data_fp32 = p.data.float()

                state = self.state[p]

                if len(state) == 0:
                    state[&#39;step&#39;] = 0
                    state[&#39;exp_avg&#39;] = torch.zeros_like(p_data_fp32)
                    state[&#39;exp_avg_sq&#39;] = torch.zeros_like(p_data_fp32)
                else:
                    state[&#39;exp_avg&#39;] = state[&#39;exp_avg&#39;].type_as(p_data_fp32)
                    state[&#39;exp_avg_sq&#39;] = state[&#39;exp_avg_sq&#39;].type_as(p_data_fp32)

                exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;]
                beta1, beta2 = group[&#39;betas&#39;]

                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
                exp_avg.mul_(beta1).add_(1 - beta1, grad)

                state[&#39;step&#39;] += 1
                beta2_t = beta2 ** state[&#39;step&#39;]
                N_sma_max = 2 / (1 - beta2) - 1
                N_sma = N_sma_max - 2 * state[&#39;step&#39;] * beta2_t / (1 - beta2_t)

                if group[&#39;weight_decay&#39;] != 0:
                    p_data_fp32.add_(-group[&#39;weight_decay&#39;] * group[&#39;lr&#39;], p_data_fp32)

                # more conservative since it&#39;s an approximated value
                if N_sma &gt;= 5:                    
                    step_size = group[&#39;lr&#39;] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state[&#39;step&#39;])
                    denom = exp_avg_sq.sqrt().add_(group[&#39;eps&#39;])
                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)
                else:
                    step_size = group[&#39;lr&#39;] / (1 - beta1 ** state[&#39;step&#39;])
                    p_data_fp32.add_(-step_size, exp_avg)

                p.data.copy_(p_data_fp32)

        return loss


class AdamW(Optimizer):

    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, warmup = 0):
        defaults = dict(lr=lr, betas=betas, eps=eps,
                        weight_decay=weight_decay, warmup = warmup)
        super(AdamW, self).__init__(params, defaults)

    def __setstate__(self, state):
        super(AdamW, self).__setstate__(state)

    def step(self, closure=None):
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:

            for p in group[&#39;params&#39;]:
                if p.grad is None:
                    continue
                grad = p.grad.data.float()
                if grad.is_sparse:
                    raise RuntimeError(&#39;Adam does not support sparse gradients, please consider SparseAdam instead&#39;)

                p_data_fp32 = p.data.float()

                state = self.state[p]

                if len(state) == 0:
                    state[&#39;step&#39;] = 0
                    state[&#39;exp_avg&#39;] = torch.zeros_like(p_data_fp32)
                    state[&#39;exp_avg_sq&#39;] = torch.zeros_like(p_data_fp32)
                else:
                    state[&#39;exp_avg&#39;] = state[&#39;exp_avg&#39;].type_as(p_data_fp32)
                    state[&#39;exp_avg_sq&#39;] = state[&#39;exp_avg_sq&#39;].type_as(p_data_fp32)

                exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;]
                beta1, beta2 = group[&#39;betas&#39;]

                state[&#39;step&#39;] += 1

                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
                exp_avg.mul_(beta1).add_(1 - beta1, grad)

                denom = exp_avg_sq.sqrt().add_(group[&#39;eps&#39;])
                bias_correction1 = 1 - beta1 ** state[&#39;step&#39;]
                bias_correction2 = 1 - beta2 ** state[&#39;step&#39;]
                
                if group[&#39;warmup&#39;] &gt; state[&#39;step&#39;]:
                    scheduled_lr = 1e-8 + state[&#39;step&#39;] * group[&#39;lr&#39;] / group[&#39;warmup&#39;]
                else:
                    scheduled_lr = group[&#39;lr&#39;]

                step_size = group[&#39;lr&#39;] * math.sqrt(bias_correction2) / bias_correction1
                
                if group[&#39;weight_decay&#39;] != 0:
                    p_data_fp32.add_(-group[&#39;weight_decay&#39;] * scheduled_lr, p_data_fp32)

                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)

                p.data.copy_(p_data_fp32)

        return loss</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="autoENRICH.ml.models.BCAI.modules.radam.AdamW"><code class="flex name class">
<span>class <span class="ident">AdamW</span></span>
<span>(</span><span>params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, warmup=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Base class for all optimizers.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Parameters need to be specified as collections that have a deterministic
ordering that is consistent between runs. Examples of objects that don't
satisfy those properties are sets and iterators over values of dictionaries.</p>
</div>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>iterable</code></dt>
<dd>an iterable of :class:<code>torch.Tensor</code> s or
:class:<code>dict</code> s. Specifies what Tensors should be optimized.</dd>
<dt><strong><code>defaults</code></strong></dt>
<dd>(dict): a dict containing default values of optimization
options (used when a parameter group doesn't specify them).</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AdamW(Optimizer):

    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, warmup = 0):
        defaults = dict(lr=lr, betas=betas, eps=eps,
                        weight_decay=weight_decay, warmup = warmup)
        super(AdamW, self).__init__(params, defaults)

    def __setstate__(self, state):
        super(AdamW, self).__setstate__(state)

    def step(self, closure=None):
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:

            for p in group[&#39;params&#39;]:
                if p.grad is None:
                    continue
                grad = p.grad.data.float()
                if grad.is_sparse:
                    raise RuntimeError(&#39;Adam does not support sparse gradients, please consider SparseAdam instead&#39;)

                p_data_fp32 = p.data.float()

                state = self.state[p]

                if len(state) == 0:
                    state[&#39;step&#39;] = 0
                    state[&#39;exp_avg&#39;] = torch.zeros_like(p_data_fp32)
                    state[&#39;exp_avg_sq&#39;] = torch.zeros_like(p_data_fp32)
                else:
                    state[&#39;exp_avg&#39;] = state[&#39;exp_avg&#39;].type_as(p_data_fp32)
                    state[&#39;exp_avg_sq&#39;] = state[&#39;exp_avg_sq&#39;].type_as(p_data_fp32)

                exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;]
                beta1, beta2 = group[&#39;betas&#39;]

                state[&#39;step&#39;] += 1

                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
                exp_avg.mul_(beta1).add_(1 - beta1, grad)

                denom = exp_avg_sq.sqrt().add_(group[&#39;eps&#39;])
                bias_correction1 = 1 - beta1 ** state[&#39;step&#39;]
                bias_correction2 = 1 - beta2 ** state[&#39;step&#39;]
                
                if group[&#39;warmup&#39;] &gt; state[&#39;step&#39;]:
                    scheduled_lr = 1e-8 + state[&#39;step&#39;] * group[&#39;lr&#39;] / group[&#39;warmup&#39;]
                else:
                    scheduled_lr = group[&#39;lr&#39;]

                step_size = group[&#39;lr&#39;] * math.sqrt(bias_correction2) / bias_correction1
                
                if group[&#39;weight_decay&#39;] != 0:
                    p_data_fp32.add_(-group[&#39;weight_decay&#39;] * scheduled_lr, p_data_fp32)

                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)

                p.data.copy_(p_data_fp32)

        return loss</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.optim.optimizer.Optimizer</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="autoENRICH.ml.models.BCAI.modules.radam.AdamW.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, closure=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Performs a single optimization step (parameter update).</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>closure</code></strong> :&ensp;<code>callable</code></dt>
<dd>A closure that reevaluates the model and
returns the loss. Optional for most optimizers.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, closure=None):
    loss = None
    if closure is not None:
        loss = closure()

    for group in self.param_groups:

        for p in group[&#39;params&#39;]:
            if p.grad is None:
                continue
            grad = p.grad.data.float()
            if grad.is_sparse:
                raise RuntimeError(&#39;Adam does not support sparse gradients, please consider SparseAdam instead&#39;)

            p_data_fp32 = p.data.float()

            state = self.state[p]

            if len(state) == 0:
                state[&#39;step&#39;] = 0
                state[&#39;exp_avg&#39;] = torch.zeros_like(p_data_fp32)
                state[&#39;exp_avg_sq&#39;] = torch.zeros_like(p_data_fp32)
            else:
                state[&#39;exp_avg&#39;] = state[&#39;exp_avg&#39;].type_as(p_data_fp32)
                state[&#39;exp_avg_sq&#39;] = state[&#39;exp_avg_sq&#39;].type_as(p_data_fp32)

            exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;]
            beta1, beta2 = group[&#39;betas&#39;]

            state[&#39;step&#39;] += 1

            exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
            exp_avg.mul_(beta1).add_(1 - beta1, grad)

            denom = exp_avg_sq.sqrt().add_(group[&#39;eps&#39;])
            bias_correction1 = 1 - beta1 ** state[&#39;step&#39;]
            bias_correction2 = 1 - beta2 ** state[&#39;step&#39;]
            
            if group[&#39;warmup&#39;] &gt; state[&#39;step&#39;]:
                scheduled_lr = 1e-8 + state[&#39;step&#39;] * group[&#39;lr&#39;] / group[&#39;warmup&#39;]
            else:
                scheduled_lr = group[&#39;lr&#39;]

            step_size = group[&#39;lr&#39;] * math.sqrt(bias_correction2) / bias_correction1
            
            if group[&#39;weight_decay&#39;] != 0:
                p_data_fp32.add_(-group[&#39;weight_decay&#39;] * scheduled_lr, p_data_fp32)

            p_data_fp32.addcdiv_(-step_size, exp_avg, denom)

            p.data.copy_(p_data_fp32)

    return loss</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="autoENRICH.ml.models.BCAI.modules.radam.PlainRAdam"><code class="flex name class">
<span>class <span class="ident">PlainRAdam</span></span>
<span>(</span><span>params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Base class for all optimizers.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Parameters need to be specified as collections that have a deterministic
ordering that is consistent between runs. Examples of objects that don't
satisfy those properties are sets and iterators over values of dictionaries.</p>
</div>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>iterable</code></dt>
<dd>an iterable of :class:<code>torch.Tensor</code> s or
:class:<code>dict</code> s. Specifies what Tensors should be optimized.</dd>
<dt><strong><code>defaults</code></strong></dt>
<dd>(dict): a dict containing default values of optimization
options (used when a parameter group doesn't specify them).</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PlainRAdam(Optimizer):

    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)

        super(PlainRAdam, self).__init__(params, defaults)

    def __setstate__(self, state):
        super(PlainRAdam, self).__setstate__(state)

    def step(self, closure=None):

        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:

            for p in group[&#39;params&#39;]:
                if p.grad is None:
                    continue
                grad = p.grad.data.float()
                if grad.is_sparse:
                    raise RuntimeError(&#39;RAdam does not support sparse gradients&#39;)

                p_data_fp32 = p.data.float()

                state = self.state[p]

                if len(state) == 0:
                    state[&#39;step&#39;] = 0
                    state[&#39;exp_avg&#39;] = torch.zeros_like(p_data_fp32)
                    state[&#39;exp_avg_sq&#39;] = torch.zeros_like(p_data_fp32)
                else:
                    state[&#39;exp_avg&#39;] = state[&#39;exp_avg&#39;].type_as(p_data_fp32)
                    state[&#39;exp_avg_sq&#39;] = state[&#39;exp_avg_sq&#39;].type_as(p_data_fp32)

                exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;]
                beta1, beta2 = group[&#39;betas&#39;]

                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
                exp_avg.mul_(beta1).add_(1 - beta1, grad)

                state[&#39;step&#39;] += 1
                beta2_t = beta2 ** state[&#39;step&#39;]
                N_sma_max = 2 / (1 - beta2) - 1
                N_sma = N_sma_max - 2 * state[&#39;step&#39;] * beta2_t / (1 - beta2_t)

                if group[&#39;weight_decay&#39;] != 0:
                    p_data_fp32.add_(-group[&#39;weight_decay&#39;] * group[&#39;lr&#39;], p_data_fp32)

                # more conservative since it&#39;s an approximated value
                if N_sma &gt;= 5:                    
                    step_size = group[&#39;lr&#39;] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state[&#39;step&#39;])
                    denom = exp_avg_sq.sqrt().add_(group[&#39;eps&#39;])
                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)
                else:
                    step_size = group[&#39;lr&#39;] / (1 - beta1 ** state[&#39;step&#39;])
                    p_data_fp32.add_(-step_size, exp_avg)

                p.data.copy_(p_data_fp32)

        return loss</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.optim.optimizer.Optimizer</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="autoENRICH.ml.models.BCAI.modules.radam.PlainRAdam.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, closure=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Performs a single optimization step (parameter update).</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>closure</code></strong> :&ensp;<code>callable</code></dt>
<dd>A closure that reevaluates the model and
returns the loss. Optional for most optimizers.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, closure=None):

    loss = None
    if closure is not None:
        loss = closure()

    for group in self.param_groups:

        for p in group[&#39;params&#39;]:
            if p.grad is None:
                continue
            grad = p.grad.data.float()
            if grad.is_sparse:
                raise RuntimeError(&#39;RAdam does not support sparse gradients&#39;)

            p_data_fp32 = p.data.float()

            state = self.state[p]

            if len(state) == 0:
                state[&#39;step&#39;] = 0
                state[&#39;exp_avg&#39;] = torch.zeros_like(p_data_fp32)
                state[&#39;exp_avg_sq&#39;] = torch.zeros_like(p_data_fp32)
            else:
                state[&#39;exp_avg&#39;] = state[&#39;exp_avg&#39;].type_as(p_data_fp32)
                state[&#39;exp_avg_sq&#39;] = state[&#39;exp_avg_sq&#39;].type_as(p_data_fp32)

            exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;]
            beta1, beta2 = group[&#39;betas&#39;]

            exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
            exp_avg.mul_(beta1).add_(1 - beta1, grad)

            state[&#39;step&#39;] += 1
            beta2_t = beta2 ** state[&#39;step&#39;]
            N_sma_max = 2 / (1 - beta2) - 1
            N_sma = N_sma_max - 2 * state[&#39;step&#39;] * beta2_t / (1 - beta2_t)

            if group[&#39;weight_decay&#39;] != 0:
                p_data_fp32.add_(-group[&#39;weight_decay&#39;] * group[&#39;lr&#39;], p_data_fp32)

            # more conservative since it&#39;s an approximated value
            if N_sma &gt;= 5:                    
                step_size = group[&#39;lr&#39;] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state[&#39;step&#39;])
                denom = exp_avg_sq.sqrt().add_(group[&#39;eps&#39;])
                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)
            else:
                step_size = group[&#39;lr&#39;] / (1 - beta1 ** state[&#39;step&#39;])
                p_data_fp32.add_(-step_size, exp_avg)

            p.data.copy_(p_data_fp32)

    return loss</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="autoENRICH.ml.models.BCAI.modules.radam.RAdam"><code class="flex name class">
<span>class <span class="ident">RAdam</span></span>
<span>(</span><span>params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Base class for all optimizers.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Parameters need to be specified as collections that have a deterministic
ordering that is consistent between runs. Examples of objects that don't
satisfy those properties are sets and iterators over values of dictionaries.</p>
</div>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>iterable</code></dt>
<dd>an iterable of :class:<code>torch.Tensor</code> s or
:class:<code>dict</code> s. Specifies what Tensors should be optimized.</dd>
<dt><strong><code>defaults</code></strong></dt>
<dd>(dict): a dict containing default values of optimization
options (used when a parameter group doesn't specify them).</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RAdam(Optimizer):

    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        self.buffer = [[None, None, None] for ind in range(10)]
        super(RAdam, self).__init__(params, defaults)

    def __setstate__(self, state):
        super(RAdam, self).__setstate__(state)

    def step(self, closure=None):

        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:

            for p in group[&#39;params&#39;]:
                if p.grad is None:
                    continue
                grad = p.grad.data.float()
                if grad.is_sparse:
                    raise RuntimeError(&#39;RAdam does not support sparse gradients&#39;)

                p_data_fp32 = p.data.float()

                state = self.state[p]

                if len(state) == 0:
                    state[&#39;step&#39;] = 0
                    state[&#39;exp_avg&#39;] = torch.zeros_like(p_data_fp32)
                    state[&#39;exp_avg_sq&#39;] = torch.zeros_like(p_data_fp32)
                else:
                    state[&#39;exp_avg&#39;] = state[&#39;exp_avg&#39;].type_as(p_data_fp32)
                    state[&#39;exp_avg_sq&#39;] = state[&#39;exp_avg_sq&#39;].type_as(p_data_fp32)

                exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;]
                beta1, beta2 = group[&#39;betas&#39;]

                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
                exp_avg.mul_(beta1).add_(1 - beta1, grad)

                state[&#39;step&#39;] += 1
                buffered = self.buffer[int(state[&#39;step&#39;] % 10)]
                if state[&#39;step&#39;] == buffered[0]:
                    N_sma, step_size = buffered[1], buffered[2]
                else:
                    buffered[0] = state[&#39;step&#39;]
                    beta2_t = beta2 ** state[&#39;step&#39;]
                    N_sma_max = 2 / (1 - beta2) - 1
                    N_sma = N_sma_max - 2 * state[&#39;step&#39;] * beta2_t / (1 - beta2_t)
                    buffered[1] = N_sma

                    # more conservative since it&#39;s an approximated value
                    if N_sma &gt;= 5:
                        step_size = group[&#39;lr&#39;] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state[&#39;step&#39;])
                    else:
                        step_size = group[&#39;lr&#39;] / (1 - beta1 ** state[&#39;step&#39;])
                    buffered[2] = step_size

                if group[&#39;weight_decay&#39;] != 0:
                    p_data_fp32.add_(-group[&#39;weight_decay&#39;] * group[&#39;lr&#39;], p_data_fp32)

                # more conservative since it&#39;s an approximated value
                if N_sma &gt;= 5:            
                    denom = exp_avg_sq.sqrt().add_(group[&#39;eps&#39;])
                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)
                else:
                    p_data_fp32.add_(-step_size, exp_avg)

                p.data.copy_(p_data_fp32)

        return loss</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.optim.optimizer.Optimizer</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="autoENRICH.ml.models.BCAI.modules.radam.RAdam.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, closure=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Performs a single optimization step (parameter update).</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>closure</code></strong> :&ensp;<code>callable</code></dt>
<dd>A closure that reevaluates the model and
returns the loss. Optional for most optimizers.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, closure=None):

    loss = None
    if closure is not None:
        loss = closure()

    for group in self.param_groups:

        for p in group[&#39;params&#39;]:
            if p.grad is None:
                continue
            grad = p.grad.data.float()
            if grad.is_sparse:
                raise RuntimeError(&#39;RAdam does not support sparse gradients&#39;)

            p_data_fp32 = p.data.float()

            state = self.state[p]

            if len(state) == 0:
                state[&#39;step&#39;] = 0
                state[&#39;exp_avg&#39;] = torch.zeros_like(p_data_fp32)
                state[&#39;exp_avg_sq&#39;] = torch.zeros_like(p_data_fp32)
            else:
                state[&#39;exp_avg&#39;] = state[&#39;exp_avg&#39;].type_as(p_data_fp32)
                state[&#39;exp_avg_sq&#39;] = state[&#39;exp_avg_sq&#39;].type_as(p_data_fp32)

            exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;]
            beta1, beta2 = group[&#39;betas&#39;]

            exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
            exp_avg.mul_(beta1).add_(1 - beta1, grad)

            state[&#39;step&#39;] += 1
            buffered = self.buffer[int(state[&#39;step&#39;] % 10)]
            if state[&#39;step&#39;] == buffered[0]:
                N_sma, step_size = buffered[1], buffered[2]
            else:
                buffered[0] = state[&#39;step&#39;]
                beta2_t = beta2 ** state[&#39;step&#39;]
                N_sma_max = 2 / (1 - beta2) - 1
                N_sma = N_sma_max - 2 * state[&#39;step&#39;] * beta2_t / (1 - beta2_t)
                buffered[1] = N_sma

                # more conservative since it&#39;s an approximated value
                if N_sma &gt;= 5:
                    step_size = group[&#39;lr&#39;] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state[&#39;step&#39;])
                else:
                    step_size = group[&#39;lr&#39;] / (1 - beta1 ** state[&#39;step&#39;])
                buffered[2] = step_size

            if group[&#39;weight_decay&#39;] != 0:
                p_data_fp32.add_(-group[&#39;weight_decay&#39;] * group[&#39;lr&#39;], p_data_fp32)

            # more conservative since it&#39;s an approximated value
            if N_sma &gt;= 5:            
                denom = exp_avg_sq.sqrt().add_(group[&#39;eps&#39;])
                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)
            else:
                p_data_fp32.add_(-step_size, exp_avg)

            p.data.copy_(p_data_fp32)

    return loss</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="autoENRICH.ml.models.BCAI.modules" href="index.html">autoENRICH.ml.models.BCAI.modules</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="autoENRICH.ml.models.BCAI.modules.radam.AdamW" href="#autoENRICH.ml.models.BCAI.modules.radam.AdamW">AdamW</a></code></h4>
<ul class="">
<li><code><a title="autoENRICH.ml.models.BCAI.modules.radam.AdamW.step" href="#autoENRICH.ml.models.BCAI.modules.radam.AdamW.step">step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="autoENRICH.ml.models.BCAI.modules.radam.PlainRAdam" href="#autoENRICH.ml.models.BCAI.modules.radam.PlainRAdam">PlainRAdam</a></code></h4>
<ul class="">
<li><code><a title="autoENRICH.ml.models.BCAI.modules.radam.PlainRAdam.step" href="#autoENRICH.ml.models.BCAI.modules.radam.PlainRAdam.step">step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="autoENRICH.ml.models.BCAI.modules.radam.RAdam" href="#autoENRICH.ml.models.BCAI.modules.radam.RAdam">RAdam</a></code></h4>
<ul class="">
<li><code><a title="autoENRICH.ml.models.BCAI.modules.radam.RAdam.step" href="#autoENRICH.ml.models.BCAI.modules.radam.RAdam.step">step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>