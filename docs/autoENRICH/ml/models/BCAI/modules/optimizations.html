<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.4" />
<title>autoENRICH.ml.models.BCAI.modules.optimizations API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>autoENRICH.ml.models.BCAI.modules.optimizations</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">## This source code is from TrellisNet written by CMU Locus Lab
##   (https://github.com/locuslab/trellisnet/tree/master)
##
##
##MIT License
##
##Copyright (c) 2018 CMU Locus Lab
##
##Permission is hereby granted, free of charge, to any person obtaining a copy
##of this software and associated documentation files (the &#34;Software&#34;), to deal
##in the Software without restriction, including without limitation the rights
##to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
##copies of the Software, and to permit persons to whom the Software is
##furnished to do so, subject to the following conditions:
##
##The above copyright notice and this permission notice shall be included in all
##copies or substantial portions of the Software.
##
##THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
##IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
##FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
##AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
##LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
##OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
##SOFTWARE.

from torch.nn.parameter import Parameter
import torch.nn as nn
import torch
import torch.nn.functional as F
from torch.autograd import Variable

##############################################################################################################
#
# Temporal DropConnect in a feed-forward setting
#
##############################################################################################################

class WeightDrop(torch.nn.Module):
    def __init__(self, module, weights, dropout=0, temporal=False):
        &#34;&#34;&#34;
        Weight DropConnect, adapted from a recurrent setting by Merity et al. 2017

        :param module: The module whose weights are to be applied dropout on
        :param weights: A 2D list identifying the weights to be regularized. Each element of weights should be a
                        list containing the &#34;path&#34; to the weight kernel. For instance, if we want to regularize
                        module.layer2.weight3, then this should be [&#34;layer2&#34;, &#34;weight3&#34;].
        :param dropout: The dropout rate (0 means no dropout)
        :param temporal: Whether we apply DropConnect only to the temporal parts of the weight (empirically we found
                         this not very important)
        &#34;&#34;&#34;
        super(WeightDrop, self).__init__()
        self.module = module
        self.weights = weights
        self.dropout = dropout
        self.temporal = temporal
        self._setup()

    def _setup(self):
        for path in self.weights:
            full_name_w = &#39;.&#39;.join(path)

            module = self.module
            name_w = path[-1]
            for i in range(len(path) - 1):
                module = getattr(module, path[i])
            w = getattr(module, name_w)
            del module._parameters[name_w]
            module.register_parameter(name_w + &#39;_raw&#39;, Parameter(w.data))

    def _setweights(self):
        for path in self.weights:
            module = self.module
            name_w = path[-1]
            for i in range(len(path) - 1):
                module = getattr(module, path[i])
            raw_w = getattr(module, name_w + &#39;_raw&#39;)

            if len(raw_w.size()) &gt; 2 and raw_w.size(2) &gt; 1 and self.temporal:
                # Drop the temporal parts of the weight; if 1x1 convolution then drop the whole kernel
                w = torch.cat([F.dropout(raw_w[:, :, :-1], p=self.dropout, training=self.training),
                               raw_w[:, :, -1:]], dim=2)
            else:
                w = F.dropout(raw_w, p=self.dropout, training=self.training)

            setattr(module, name_w, w)

    def forward(self, *args, **kwargs):
        self._setweights()
        return self.module.forward(*args, **kwargs)


def matrix_diag(a, dim=2):
    &#34;&#34;&#34;
    a has dimension (N, (L,) C), we want a matrix/batch diag that produces (N, (L,) C, C) from the last dimension of a
    &#34;&#34;&#34;
    if dim == 2:
        res = torch.zeros(a.size(0), a.size(1), a.size(1))
        res.as_strided(a.size(), [res.stride(0), res.size(2)+1]).copy_(a)
    else:
        res = torch.zeros(a.size(0), a.size(1), a.size(2), a.size(2))
        res.as_strided(a.size(), [res.stride(0), res.stride(1), res.size(3)+1]).copy_(a)
    return res

##############################################################################################################
#
# Embedding dropout
#
##############################################################################################################

def embedded_dropout(embed, words, dropout=0.1, scale=None):
    &#34;&#34;&#34;
    Apply embedding encoder (whose weight we apply a dropout)

    :param embed: The embedding layer
    :param words: The input sequence
    :param dropout: The embedding weight dropout rate
    :param scale: Scaling factor for the dropped embedding weight
    :return: The embedding output
    &#34;&#34;&#34;
    if dropout:
        mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(
            embed.weight) / (1 - dropout)
        mask = Variable(mask)
        masked_embed_weight = mask * embed.weight
    else:
        masked_embed_weight = embed.weight

    if scale:
        masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight

    padding_idx = embed.padding_idx
    if padding_idx is None:
        padding_idx = -1

    # Handle PyTorch issue
    if &#39;0.3&#39; not in torch.__version__:
        X = F.embedding(
            words, masked_embed_weight,
            padding_idx,
            embed.max_norm, embed.norm_type,
            embed.scale_grad_by_freq, embed.sparse
        )
    else:
        X = embed._backend.Embedding.apply(words, masked_embed_weight,
                                           padding_idx, embed.max_norm, embed.norm_type,
                                           embed.scale_grad_by_freq, embed.sparse
                                           )
    return X



##############################################################################################################
#
# Variational dropout (for input/output layers, and for hidden layers)
#
##############################################################################################################

class VariationalDropout(nn.Module):
    def __init__(self):
        &#34;&#34;&#34;
        Feed-forward version of variational dropout that applies the same mask at every time step
        &#34;&#34;&#34;
        super(VariationalDropout, self).__init__()

    def forward(self, x, dropout=0.5, dim=3):
        if not self.training or not dropout:
            return x
        if dim == 4:
            # Dimension (M, N, L, C), where C stands for channels
            m = torch.zeros_like(x[:,:,:1]).bernoulli_(1 - dropout)
        else:
            # Dimension (N, L, C)
            m = torch.zeros_like(x[:,:1]).bernoulli_(1 - dropout)
        mask = m.requires_grad_(False) / (1 - dropout)
        mask = mask.expand_as(x)
        return mask * x


class VariationalHidDropout(nn.Module):
    def __init__(self, dropout=0.0, channel_axis=1):
        &#34;&#34;&#34;
        Hidden-to-hidden (VD-based) dropout that applies the same mask at every time step and every layer of TrellisNet
        :param dropout: The dropout rate (0 means no dropout is applied)
        :param temporal: Whether the dropout mask is the same across the temporal dimension (or only the depth dimension)
        &#34;&#34;&#34;
        super(VariationalHidDropout, self).__init__()
        self.dropout = dropout
        self.mask = None
        self.channel_axis = channel_axis
        
    def reset_mask(self, x):
        dropout = self.dropout

        # Dimension (N, C, L) if channel_axis == 1 else (N, L, C)
        base_shape = x[:,:,:1] if self.channel_axis == 1 else x[:,:1]
        m = torch.zeros_like(base_shape).bernoulli_(1 - dropout)
        mask = m.requires_grad_(False) / (1 - dropout)
        self.mask = mask
        return mask

    def forward(self, x):
        if not self.training or self.dropout == 0:
            return x
        assert self.mask is not None, &#34;You need to reset mask before using VariationalHidDropout&#34;
        mask = self.mask.expand_as(x)  # Make sure the dimension matches
        return mask * x
    
    
class VariationalAttnDropout(VariationalHidDropout):
    def __init__(self, dropout=0.0, temporal=True):
        super(VariationalAttnDropout, self).__init__(dropout)

    def reset_mask(self, x):
        # Dimension (N, n_head, L1, L2)
        m = torch.zeros_like(x).bernoulli_(1 - self.dropout)
        mask = m.requires_grad_(False) / (1 - self.dropout)
        self.mask = mask
        return mask



##############################################################################################################
#
# Weight normalization. Modified from the original PyTorch&#39;s implementation of weight normalization.
#
##############################################################################################################

def _norm(p, dim):
    &#34;&#34;&#34;Computes the norm over all dimensions except dim&#34;&#34;&#34;
    if dim is None:
        return p.norm()
    elif dim == 0:
        output_size = (p.size(0),) + (1,) * (p.dim() - 1)
        return p.contiguous().view(p.size(0), -1).norm(dim=1).view(*output_size)
    elif dim == p.dim() - 1:
        output_size = (1,) * (p.dim() - 1) + (p.size(-1),)
        return p.contiguous().view(-1, p.size(-1)).norm(dim=0).view(*output_size)
    else:
        return _norm(p.transpose(0, dim), 0).transpose(0, dim)


class WeightNorm(object):
    def __init__(self, names, dim):
        &#34;&#34;&#34;
        Weight normalization module

        :param names: The list of weight names to apply weightnorm on
        :param dim: The dimension of the weights to be normalized
        &#34;&#34;&#34;
        self.names = names
        self.dim = dim

    def compute_weight(self, module, name):
        g = getattr(module, name + &#39;_g&#39;)
        v = getattr(module, name + &#39;_v&#39;)
        return v * (g / _norm(v, self.dim))

    @staticmethod
    def apply(module, names, dim):
        fn = WeightNorm(names, dim)

        for name in names:
            weight = getattr(module, name)

            # remove w from parameter list
            del module._parameters[name]

            # add g and v as new parameters and express w as g/||v|| * v
            module.register_parameter(name + &#39;_g&#39;, Parameter(_norm(weight, dim).data))
            module.register_parameter(name + &#39;_v&#39;, Parameter(weight.data))
            setattr(module, name, fn.compute_weight(module, name))

        # recompute weight before every forward()
        module.register_forward_pre_hook(fn)
        return fn

    def remove(self, module):
        for name in self.names:
            weight = self.compute_weight(module, name)
            delattr(module, name)
            del module._parameters[name + &#39;_g&#39;]
            del module._parameters[name + &#39;_v&#39;]
            module.register_parameter(name, Parameter(weight.data))

    def reset(self, module):
        for name in self.names:
            setattr(module, name, self.compute_weight(module, name))

    def __call__(self, module, inputs):
        # Typically, every time the module is called we need to recompute the weight. However,
        # in the case of TrellisNet, the same weight is shared across layers, and we can save
        # a lot of intermediate memory by just recomputing once (at the beginning of first call).
        pass


def weight_norm(module, names, dim=0):
    fn = WeightNorm.apply(module, names, dim)
    return module, fn</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.embedded_dropout"><code class="name flex">
<span>def <span class="ident">embedded_dropout</span></span>(<span>embed, words, dropout=0.1, scale=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Apply embedding encoder (whose weight we apply a dropout)</p>
<p>:param embed: The embedding layer
:param words: The input sequence
:param dropout: The embedding weight dropout rate
:param scale: Scaling factor for the dropped embedding weight
:return: The embedding output</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def embedded_dropout(embed, words, dropout=0.1, scale=None):
    &#34;&#34;&#34;
    Apply embedding encoder (whose weight we apply a dropout)

    :param embed: The embedding layer
    :param words: The input sequence
    :param dropout: The embedding weight dropout rate
    :param scale: Scaling factor for the dropped embedding weight
    :return: The embedding output
    &#34;&#34;&#34;
    if dropout:
        mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(
            embed.weight) / (1 - dropout)
        mask = Variable(mask)
        masked_embed_weight = mask * embed.weight
    else:
        masked_embed_weight = embed.weight

    if scale:
        masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight

    padding_idx = embed.padding_idx
    if padding_idx is None:
        padding_idx = -1

    # Handle PyTorch issue
    if &#39;0.3&#39; not in torch.__version__:
        X = F.embedding(
            words, masked_embed_weight,
            padding_idx,
            embed.max_norm, embed.norm_type,
            embed.scale_grad_by_freq, embed.sparse
        )
    else:
        X = embed._backend.Embedding.apply(words, masked_embed_weight,
                                           padding_idx, embed.max_norm, embed.norm_type,
                                           embed.scale_grad_by_freq, embed.sparse
                                           )
    return X</code></pre>
</details>
</dd>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.matrix_diag"><code class="name flex">
<span>def <span class="ident">matrix_diag</span></span>(<span>a, dim=2)</span>
</code></dt>
<dd>
<section class="desc"><p>a has dimension (N, (L,) C), we want a matrix/batch diag that produces (N, (L,) C, C) from the last dimension of a</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def matrix_diag(a, dim=2):
    &#34;&#34;&#34;
    a has dimension (N, (L,) C), we want a matrix/batch diag that produces (N, (L,) C, C) from the last dimension of a
    &#34;&#34;&#34;
    if dim == 2:
        res = torch.zeros(a.size(0), a.size(1), a.size(1))
        res.as_strided(a.size(), [res.stride(0), res.size(2)+1]).copy_(a)
    else:
        res = torch.zeros(a.size(0), a.size(1), a.size(2), a.size(2))
        res.as_strided(a.size(), [res.stride(0), res.stride(1), res.size(3)+1]).copy_(a)
    return res</code></pre>
</details>
</dd>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.weight_norm"><code class="name flex">
<span>def <span class="ident">weight_norm</span></span>(<span>module, names, dim=0)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def weight_norm(module, names, dim=0):
    fn = WeightNorm.apply(module, names, dim)
    return module, fn</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalAttnDropout"><code class="flex name class">
<span>class <span class="ident">VariationalAttnDropout</span></span>
<span>(</span><span>dropout=0.0, temporal=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>Hidden-to-hidden (VD-based) dropout that applies the same mask at every time step and every layer of TrellisNet
:param dropout: The dropout rate (0 means no dropout is applied)
:param temporal: Whether the dropout mask is the same across the temporal dimension (or only the depth dimension)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VariationalAttnDropout(VariationalHidDropout):
    def __init__(self, dropout=0.0, temporal=True):
        super(VariationalAttnDropout, self).__init__(dropout)

    def reset_mask(self, x):
        # Dimension (N, n_head, L1, L2)
        m = torch.zeros_like(x).bernoulli_(1 - self.dropout)
        mask = m.requires_grad_(False) / (1 - self.dropout)
        self.mask = mask
        return mask</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalHidDropout" href="#autoENRICH.ml.models.BCAI.modules.optimizations.VariationalHidDropout">VariationalHidDropout</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalAttnDropout.reset_mask"><code class="name flex">
<span>def <span class="ident">reset_mask</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_mask(self, x):
    # Dimension (N, n_head, L1, L2)
    m = torch.zeros_like(x).bernoulli_(1 - self.dropout)
    mask = m.requires_grad_(False) / (1 - self.dropout)
    self.mask = mask
    return mask</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalHidDropout" href="#autoENRICH.ml.models.BCAI.modules.optimizations.VariationalHidDropout">VariationalHidDropout</a></b></code>:
<ul class="hlist">
<li><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalHidDropout.forward" href="#autoENRICH.ml.models.BCAI.modules.optimizations.VariationalHidDropout.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalDropout"><code class="flex name class">
<span>class <span class="ident">VariationalDropout</span></span>
</code></dt>
<dd>
<section class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>Feed-forward version of variational dropout that applies the same mask at every time step</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VariationalDropout(nn.Module):
    def __init__(self):
        &#34;&#34;&#34;
        Feed-forward version of variational dropout that applies the same mask at every time step
        &#34;&#34;&#34;
        super(VariationalDropout, self).__init__()

    def forward(self, x, dropout=0.5, dim=3):
        if not self.training or not dropout:
            return x
        if dim == 4:
            # Dimension (M, N, L, C), where C stands for channels
            m = torch.zeros_like(x[:,:,:1]).bernoulli_(1 - dropout)
        else:
            # Dimension (N, L, C)
            m = torch.zeros_like(x[:,:1]).bernoulli_(1 - dropout)
        mask = m.requires_grad_(False) / (1 - dropout)
        mask = mask.expand_as(x)
        return mask * x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalDropout.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x, dropout=0.5, dim=3)</span>
</code></dt>
<dd>
<section class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x, dropout=0.5, dim=3):
    if not self.training or not dropout:
        return x
    if dim == 4:
        # Dimension (M, N, L, C), where C stands for channels
        m = torch.zeros_like(x[:,:,:1]).bernoulli_(1 - dropout)
    else:
        # Dimension (N, L, C)
        m = torch.zeros_like(x[:,:1]).bernoulli_(1 - dropout)
    mask = m.requires_grad_(False) / (1 - dropout)
    mask = mask.expand_as(x)
    return mask * x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalHidDropout"><code class="flex name class">
<span>class <span class="ident">VariationalHidDropout</span></span>
<span>(</span><span>dropout=0.0, channel_axis=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>Hidden-to-hidden (VD-based) dropout that applies the same mask at every time step and every layer of TrellisNet
:param dropout: The dropout rate (0 means no dropout is applied)
:param temporal: Whether the dropout mask is the same across the temporal dimension (or only the depth dimension)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VariationalHidDropout(nn.Module):
    def __init__(self, dropout=0.0, channel_axis=1):
        &#34;&#34;&#34;
        Hidden-to-hidden (VD-based) dropout that applies the same mask at every time step and every layer of TrellisNet
        :param dropout: The dropout rate (0 means no dropout is applied)
        :param temporal: Whether the dropout mask is the same across the temporal dimension (or only the depth dimension)
        &#34;&#34;&#34;
        super(VariationalHidDropout, self).__init__()
        self.dropout = dropout
        self.mask = None
        self.channel_axis = channel_axis
        
    def reset_mask(self, x):
        dropout = self.dropout

        # Dimension (N, C, L) if channel_axis == 1 else (N, L, C)
        base_shape = x[:,:,:1] if self.channel_axis == 1 else x[:,:1]
        m = torch.zeros_like(base_shape).bernoulli_(1 - dropout)
        mask = m.requires_grad_(False) / (1 - dropout)
        self.mask = mask
        return mask

    def forward(self, x):
        if not self.training or self.dropout == 0:
            return x
        assert self.mask is not None, &#34;You need to reset mask before using VariationalHidDropout&#34;
        mask = self.mask.expand_as(x)  # Make sure the dimension matches
        return mask * x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalAttnDropout" href="#autoENRICH.ml.models.BCAI.modules.optimizations.VariationalAttnDropout">VariationalAttnDropout</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalHidDropout.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<section class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    if not self.training or self.dropout == 0:
        return x
    assert self.mask is not None, &#34;You need to reset mask before using VariationalHidDropout&#34;
    mask = self.mask.expand_as(x)  # Make sure the dimension matches
    return mask * x</code></pre>
</details>
</dd>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalHidDropout.reset_mask"><code class="name flex">
<span>def <span class="ident">reset_mask</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_mask(self, x):
    dropout = self.dropout

    # Dimension (N, C, L) if channel_axis == 1 else (N, L, C)
    base_shape = x[:,:,:1] if self.channel_axis == 1 else x[:,:1]
    m = torch.zeros_like(base_shape).bernoulli_(1 - dropout)
    mask = m.requires_grad_(False) / (1 - dropout)
    self.mask = mask
    return mask</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.WeightDrop"><code class="flex name class">
<span>class <span class="ident">WeightDrop</span></span>
<span>(</span><span>module, weights, dropout=0, temporal=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>Weight DropConnect, adapted from a recurrent setting by Merity et al. 2017</p>
<p>:param module: The module whose weights are to be applied dropout on
:param weights: A 2D list identifying the weights to be regularized. Each element of weights should be a
list containing the "path" to the weight kernel. For instance, if we want to regularize
module.layer2.weight3, then this should be ["layer2", "weight3"].
:param dropout: The dropout rate (0 means no dropout)
:param temporal: Whether we apply DropConnect only to the temporal parts of the weight (empirically we found
this not very important)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class WeightDrop(torch.nn.Module):
    def __init__(self, module, weights, dropout=0, temporal=False):
        &#34;&#34;&#34;
        Weight DropConnect, adapted from a recurrent setting by Merity et al. 2017

        :param module: The module whose weights are to be applied dropout on
        :param weights: A 2D list identifying the weights to be regularized. Each element of weights should be a
                        list containing the &#34;path&#34; to the weight kernel. For instance, if we want to regularize
                        module.layer2.weight3, then this should be [&#34;layer2&#34;, &#34;weight3&#34;].
        :param dropout: The dropout rate (0 means no dropout)
        :param temporal: Whether we apply DropConnect only to the temporal parts of the weight (empirically we found
                         this not very important)
        &#34;&#34;&#34;
        super(WeightDrop, self).__init__()
        self.module = module
        self.weights = weights
        self.dropout = dropout
        self.temporal = temporal
        self._setup()

    def _setup(self):
        for path in self.weights:
            full_name_w = &#39;.&#39;.join(path)

            module = self.module
            name_w = path[-1]
            for i in range(len(path) - 1):
                module = getattr(module, path[i])
            w = getattr(module, name_w)
            del module._parameters[name_w]
            module.register_parameter(name_w + &#39;_raw&#39;, Parameter(w.data))

    def _setweights(self):
        for path in self.weights:
            module = self.module
            name_w = path[-1]
            for i in range(len(path) - 1):
                module = getattr(module, path[i])
            raw_w = getattr(module, name_w + &#39;_raw&#39;)

            if len(raw_w.size()) &gt; 2 and raw_w.size(2) &gt; 1 and self.temporal:
                # Drop the temporal parts of the weight; if 1x1 convolution then drop the whole kernel
                w = torch.cat([F.dropout(raw_w[:, :, :-1], p=self.dropout, training=self.training),
                               raw_w[:, :, -1:]], dim=2)
            else:
                w = F.dropout(raw_w, p=self.dropout, training=self.training)

            setattr(module, name_w, w)

    def forward(self, *args, **kwargs):
        self._setweights()
        return self.module.forward(*args, **kwargs)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.WeightDrop.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, *args, **kwargs):
    self._setweights()
    return self.module.forward(*args, **kwargs)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.WeightNorm"><code class="flex name class">
<span>class <span class="ident">WeightNorm</span></span>
<span>(</span><span>names, dim)</span>
</code></dt>
<dd>
<section class="desc"><p>Weight normalization module</p>
<p>:param names: The list of weight names to apply weightnorm on
:param dim: The dimension of the weights to be normalized</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class WeightNorm(object):
    def __init__(self, names, dim):
        &#34;&#34;&#34;
        Weight normalization module

        :param names: The list of weight names to apply weightnorm on
        :param dim: The dimension of the weights to be normalized
        &#34;&#34;&#34;
        self.names = names
        self.dim = dim

    def compute_weight(self, module, name):
        g = getattr(module, name + &#39;_g&#39;)
        v = getattr(module, name + &#39;_v&#39;)
        return v * (g / _norm(v, self.dim))

    @staticmethod
    def apply(module, names, dim):
        fn = WeightNorm(names, dim)

        for name in names:
            weight = getattr(module, name)

            # remove w from parameter list
            del module._parameters[name]

            # add g and v as new parameters and express w as g/||v|| * v
            module.register_parameter(name + &#39;_g&#39;, Parameter(_norm(weight, dim).data))
            module.register_parameter(name + &#39;_v&#39;, Parameter(weight.data))
            setattr(module, name, fn.compute_weight(module, name))

        # recompute weight before every forward()
        module.register_forward_pre_hook(fn)
        return fn

    def remove(self, module):
        for name in self.names:
            weight = self.compute_weight(module, name)
            delattr(module, name)
            del module._parameters[name + &#39;_g&#39;]
            del module._parameters[name + &#39;_v&#39;]
            module.register_parameter(name, Parameter(weight.data))

    def reset(self, module):
        for name in self.names:
            setattr(module, name, self.compute_weight(module, name))

    def __call__(self, module, inputs):
        # Typically, every time the module is called we need to recompute the weight. However,
        # in the case of TrellisNet, the same weight is shared across layers, and we can save
        # a lot of intermediate memory by just recomputing once (at the beginning of first call).
        pass</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.WeightNorm.apply"><code class="name flex">
<span>def <span class="ident">apply</span></span>(<span>module, names, dim)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def apply(module, names, dim):
    fn = WeightNorm(names, dim)

    for name in names:
        weight = getattr(module, name)

        # remove w from parameter list
        del module._parameters[name]

        # add g and v as new parameters and express w as g/||v|| * v
        module.register_parameter(name + &#39;_g&#39;, Parameter(_norm(weight, dim).data))
        module.register_parameter(name + &#39;_v&#39;, Parameter(weight.data))
        setattr(module, name, fn.compute_weight(module, name))

    # recompute weight before every forward()
    module.register_forward_pre_hook(fn)
    return fn</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.WeightNorm.compute_weight"><code class="name flex">
<span>def <span class="ident">compute_weight</span></span>(<span>self, module, name)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_weight(self, module, name):
    g = getattr(module, name + &#39;_g&#39;)
    v = getattr(module, name + &#39;_v&#39;)
    return v * (g / _norm(v, self.dim))</code></pre>
</details>
</dd>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.WeightNorm.remove"><code class="name flex">
<span>def <span class="ident">remove</span></span>(<span>self, module)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove(self, module):
    for name in self.names:
        weight = self.compute_weight(module, name)
        delattr(module, name)
        del module._parameters[name + &#39;_g&#39;]
        del module._parameters[name + &#39;_v&#39;]
        module.register_parameter(name, Parameter(weight.data))</code></pre>
</details>
</dd>
<dt id="autoENRICH.ml.models.BCAI.modules.optimizations.WeightNorm.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self, module)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self, module):
    for name in self.names:
        setattr(module, name, self.compute_weight(module, name))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="autoENRICH.ml.models.BCAI.modules" href="index.html">autoENRICH.ml.models.BCAI.modules</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.embedded_dropout" href="#autoENRICH.ml.models.BCAI.modules.optimizations.embedded_dropout">embedded_dropout</a></code></li>
<li><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.matrix_diag" href="#autoENRICH.ml.models.BCAI.modules.optimizations.matrix_diag">matrix_diag</a></code></li>
<li><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.weight_norm" href="#autoENRICH.ml.models.BCAI.modules.optimizations.weight_norm">weight_norm</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalAttnDropout" href="#autoENRICH.ml.models.BCAI.modules.optimizations.VariationalAttnDropout">VariationalAttnDropout</a></code></h4>
<ul class="">
<li><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalAttnDropout.reset_mask" href="#autoENRICH.ml.models.BCAI.modules.optimizations.VariationalAttnDropout.reset_mask">reset_mask</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalDropout" href="#autoENRICH.ml.models.BCAI.modules.optimizations.VariationalDropout">VariationalDropout</a></code></h4>
<ul class="">
<li><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalDropout.forward" href="#autoENRICH.ml.models.BCAI.modules.optimizations.VariationalDropout.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalHidDropout" href="#autoENRICH.ml.models.BCAI.modules.optimizations.VariationalHidDropout">VariationalHidDropout</a></code></h4>
<ul class="">
<li><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalHidDropout.forward" href="#autoENRICH.ml.models.BCAI.modules.optimizations.VariationalHidDropout.forward">forward</a></code></li>
<li><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.VariationalHidDropout.reset_mask" href="#autoENRICH.ml.models.BCAI.modules.optimizations.VariationalHidDropout.reset_mask">reset_mask</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.WeightDrop" href="#autoENRICH.ml.models.BCAI.modules.optimizations.WeightDrop">WeightDrop</a></code></h4>
<ul class="">
<li><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.WeightDrop.forward" href="#autoENRICH.ml.models.BCAI.modules.optimizations.WeightDrop.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.WeightNorm" href="#autoENRICH.ml.models.BCAI.modules.optimizations.WeightNorm">WeightNorm</a></code></h4>
<ul class="">
<li><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.WeightNorm.apply" href="#autoENRICH.ml.models.BCAI.modules.optimizations.WeightNorm.apply">apply</a></code></li>
<li><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.WeightNorm.compute_weight" href="#autoENRICH.ml.models.BCAI.modules.optimizations.WeightNorm.compute_weight">compute_weight</a></code></li>
<li><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.WeightNorm.remove" href="#autoENRICH.ml.models.BCAI.modules.optimizations.WeightNorm.remove">remove</a></code></li>
<li><code><a title="autoENRICH.ml.models.BCAI.modules.optimizations.WeightNorm.reset" href="#autoENRICH.ml.models.BCAI.modules.optimizations.WeightNorm.reset">reset</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>