<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.4" />
<title>autoENRICH.ml.models.BCAI.predictor API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>autoENRICH.ml.models.BCAI.predictor</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python3

## Copyright (c) 2017 Robert Bosch GmbH
## All rights reserved.
##
## This source code is licensed under the MIT license found in the
## LICENSE file in the root directory of this source tree.

import bz2
import gzip
import importlib
import json
import os
import pickle
import sys

import numpy as np
import pandas as pd
from tqdm import tqdm
import torch
from torch.utils.data import TensorDataset, DataLoader

def load_model(name):
    model_folder = os.path.join(root,settings[&#39;MODEL_DIR&#39;],models[name+&#39;_dir&#39;])
    if not os.path.isdir(model_folder):
        sys.stderr.write(&#34;Error reading model from {}\n&#34;.format(model_folder))
        return None
    if not os.path.isfile(os.path.join(model_folder,&#39;model.ckpt&#39;)):
        sys.stderr.write(&#34;Error reading model from {}/model.ckpt\n&#34;.format(model_folder))
        return None
    sys.path = [model_folder] + sys.path
    import graph_transformer
    importlib.reload(graph_transformer)
    print(&#34;Loading {} from {}&#34;.format(name,graph_transformer.__file__))
    with open(os.path.join(model_folder,&#39;config&#39;)) as f:
        # JSON standard is double quotes, but some config files use single quotes.
        config_str = f.read().replace(&#34;&#39;&#34;,&#39;&#34;&#39;)
        config = json.loads(config_str)
    # Clean it up if necessary
    to_del = [&#39;name&#39;,&#39;optim&#39;,&#39;lr&#39;,&#39;mom&#39;,&#39;scheduler&#39;,&#39;warmup_step&#39;,&#39;decay_rate&#39;,
            &#39;lr_min&#39;,&#39;clip&#39;,&#39;max_epoch&#39;,&#39;batch_size&#39;,&#39;seed&#39;,&#39;cuda&#39;,&#39;debug&#39;,
            &#39;patience&#39;,&#39;champs_loss&#39;,&#39;multi_gpu&#39;,&#39;fp16&#39;,&#39;max_bond_count&#39;,
            &#39;log_interval&#39;,&#39;batch_chunk&#39;,&#39;work_dir&#39;,&#39;restart&#39;,&#39;restart_dir&#39;,
            &#39;load&#39;,&#39;mode&#39;,&#39;eta_min&#39;,&#39;gpu0_bsz&#39;,&#39;n_all_param&#39;,&#39;max_step&#39;,&#39;d_embed&#39;,
            &#39;cutout&#39;]
    for new,old in {&#39;dim&#39;:&#39;d_model&#39;, &#39;n_layers&#39;:&#39;n_layer&#39;, &#39;fdim&#39;:&#39;feature_dim&#39;,
            &#39;dist_embedding&#39;:&#39;dist_embed_type&#39;, &#39;atom_angle_embedding&#39;:&#39;angle_embed_type&#39;,
            &#39;trip_angle_embedding&#39;:&#39;quad_angle_embed_type&#39;,
            &#39;quad_angle_embedding&#39;:&#39;quad_angle_embed_type&#39;,
            }.items():
        if old in config and new not in config:
            config[new] = config[old]
            to_del.append(old)
    for old in to_del:
        if old in config:
            del config[old]
    # It would be nice to read the atom types from loaders, but it takes too long.
    config.update({k:models[k] for k in models if k.startswith(&#39;num&#39;) and k.endswith(&#39;types&#39;)})
    model = graph_transformer.GraphTransformer(**config)
    to_load_st = torch.load(os.path.join(model_folder,&#39;model.ckpt&#39;)).state_dict()
    model.load_state_dict(to_load_st)
    sys.path.remove(model_folder)
    return model


def single_model_predict(loader, model, modelname):
    MAX_BOND_COUNT = 406
    out_str = &#34;id,scalar_coupling_constant\n&#34;
    dev = &#34;cuda&#34;
    #dev = &#34;cpu&#34;
    model = model.to(dev)
    model.eval()
    with torch.no_grad():
        for arr in tqdm(loader):
            x_idx, x_atom, x_atom_pos, x_bond, x_bond_dist, x_triplet, x_triplet_angle, x_quad, x_quad_angle, y = arr
            x_atom, x_atom_pos, x_bond, x_bond_dist, x_triplet, x_triplet_angle, x_quad, x_quad_angle, y = \
                x_atom.to(dev), x_atom_pos.to(dev), x_bond.to(dev), x_bond_dist.to(dev), \
                x_triplet.to(dev), x_triplet_angle.to(dev), x_quad.to(dev), x_quad_angle.to(dev), y.to(dev)

            x_bond, x_bond_dist, y = x_bond[:, :MAX_BOND_COUNT], x_bond_dist[:, :MAX_BOND_COUNT], y[:,:MAX_BOND_COUNT]
            y_pred, _ = model(x_atom, x_atom_pos, x_bond, x_bond_dist, x_triplet, x_triplet_angle, x_quad, x_quad_angle)
            y_pred_pad = torch.cat([torch.zeros(y_pred.shape[0], 1, y_pred.shape[2], device=y_pred.device), y_pred], dim=1)
            y_pred_scaled = y_pred_pad.gather(1,x_bond[:,:,1][:,None,:])[:,0,:] * y[:,:,2] + y[:,:,1]

            y_selected = y_pred_scaled.masked_select((x_bond[:,:,0] &gt; 0) &amp; (y[:,:,3] &gt; 0)).cpu().numpy()
            ids_selected = y[:,:,0].masked_select((x_bond[:,:,0] &gt; 0) &amp; (y[:,:,3] &gt; 0))

            if dev==&#39;cuda&#39;:
                ids_selected = ids_selected.cpu()
            ids_selected = ids_selected.numpy()

            for id_, pred in zip(ids_selected, y_selected):
                out_str += &#34;{0:d},{1:f}\n&#34;.format(int(id_), pred)
    with open(os.path.join(root,settings[&#39;SUBMISSION_DIR&#39;],modelname+&#39;.csv.bz2&#39;), &#34;wb&#34;) as f:
        f.write(bz2.compress(out_str.encode(&#39;utf-8&#39;)))
    return


def load_submission(modelname):
    data = pd.read_csv(os.path.join(root,settings[&#39;SUBMISSION_DIR&#39;],modelname+&#39;.csv.bz2&#39;))
    sort_idx = np.argsort(data[&#39;id&#39;])
    out = np.vstack((data[&#39;id&#39;],data[&#39;scalar_coupling_constant&#39;]))[:,sort_idx].T
    return out


def select_models(n_model,chosen_models,hardcoded=True):
    if not hardcoded:
        model_mask = np.zeros((n_model,8), dtype=np.bool)
        for i in range(8):
            best_models = np.flip( np.argsort( chosen_models[:,i] ) )[0:model_count_for_median[i]]
            for j in best_models:
                model_mask[j,i] = True
    else:
        #assert models[&#39;names&#39;] == [&#34;gt15_3047fine3069&#34;, &#34;gt16_3044fine3068&#34;, &#34;gt18_3020&#34;, &#34;gtA_174932_3015fine3042&#34;, &#34;gtB_124323_2997&#34;, &#34;gtC_091310_3010fine3025&#34;, &#34;gtD_092424_3018fine3038&#34;, &#34;gtE_114919_2823&#34;, &#34;gtF_115725_2940&#34;, &#34;gtG_120830_3001&#34;, &#34;gtH_125215_3020&#34;, &#34;gt15_J_3049&#34;, &#34;gt14_K_3074&#34;]
        model_mask = [[ True,  True,  True,  True,  True,  True,  True,  True],  # gt15_3047fine3069
                      [ True,  True,  True,  True,  True,  True,  True,  True],  # gt16_3044fine3068
                      [ True, False,  True, False, False,  True,  True, False],  # gt18_3020
                      [ True,  True,  True,  True,  True,  True,  True,  True],  # gtA_174932_3015fine3042
                      [False,  True, False,  True,  True, False,  True,  True],  # gtB_124323_2997
                      [ True,  True,  True,  True,  True, False, False,  True],  # gtC_091310_3010fine3025
                      [ True,  True,  True,  True,  True,  True,  True,  True],  # gtD_092424_3018fine3038
                      [False, False, False, False,  True, False, False, False],  # gtE_114919_2823
                      [False, False, False,  True,  True, False, False,  True],  # gtF_115725_2940
                      [False, False, False, False, False,  True, False, False],  # gtG_120830_3001
                      [ True,  True,  True, False, False,  True,  True, False],  # gtH_125215_3020
                      [ True,  True,  True,  True, False,  True,  True,  True],  # gt15_J_3049
                      [ True,  True,  True,  True,  True,  True,  True,  True]]  # gt14_K_3074
        model_mask = np.array(model_mask, dtype=np.bool)
    return model_mask


def ensemble(modelnames):
    xs = [load_submission(name) for name in modelnames]
    n_model = len(xs)
    idx = xs[0][:,0]
    x_all = np.vstack(( [xs[i][:,1] for i in range(n_model)] ))

    with open(os.path.join(root,settings[&#39;RAW_DATA_DIR&#39;],&#39;test.csv&#39;),&#39;r&#39;) as f:
        lines = f.read().strip().split(&#39;\n&#39;)[1:]
        types = [&#39;1JHC&#39;,&#39;1JHN&#39;,&#39;2JHC&#39;,&#39;2JHH&#39;,&#39;2JHN&#39;,&#39;3JHC&#39;,&#39;3JHH&#39;,&#39;3JHN&#39;]
        line_types = np.array([types.index(line.split(&#39;,&#39;)[4]) for line in lines])
        line_type_indices = [(line_types == i) for i in range(8)]

    median_index = (n_model-1)/2
    even_models = False
    if n_model % 2 == 0:
        even_models = True
        print(&#39;WARNING: even number of models supplied&#39;)
    median_index = int(median_index)
    indices = np.argsort(x_all, axis=0)
    chosen_models = np.zeros((n_model,8), dtype=np.int)
    for i in range(n_model):
        for j in range(8):
            chosen_models[i,j] = int((indices[median_index, line_type_indices[j]] == i).sum())
            if even_models:
                chosen_models[i,j] += int((indices[median_index+1, line_type_indices[j]] == i).sum())
    if even_models:
        chosen_models = chosen_models / 2.0
    print(&#39;Count of a model &amp; type being chosen in a raw median procedure:&#39;)
    for i in range(n_model):
        print(str(chosen_models[i,:])+&#39; &#39;+modelnames[i])
    print(&#39;&#39;)

    model_mask = select_models(n_model,chosen_models)
    print(&#39;Model mask based on model count for median (now hardcoded):&#39;)
    for i in range(n_model):
        print(str(model_mask[i,:])+&#39; &#39;+modelnames[i])
    print(&#39;&#39;)

    x_out = np.zeros(x_all.shape[1])
    answer_count = 0
    for i in range(8):
        sorted_x_part = np.sort(x_all[ model_mask[:,i],: ][ :,line_type_indices[i] ], axis=0)
        model_count = model_mask[:,i].sum()
        start_idx = int((model_count - median_mean_counts[i])/2)
        end_idx = start_idx + median_mean_counts[i]
        mean_x_median = np.mean(sorted_x_part[ start_idx:end_idx,: ], axis=0)
        x_out[ line_type_indices[i] ] = mean_x_median
        print(&#39;Type &#39;+str(i)+&#39;: &#39;+str(model_count)+&#39; models average &#39;+str(median_mean_counts[i])+&#39; median&#39;)
        print(&#39;Output shape after mean: &#39;,str(mean_x_median.shape))
        print(&#39;Sorted indices for median mean: &#39;,str([j for j in range(start_idx,end_idx)]))
        answer_count += mean_x_median.shape[0]
    assert answer_count == 2505542
    return idx,x_out


def write_final(idx,x_out):
    with open(os.path.join(root,settings[&#39;SUBMISSION_DIR&#39;],models[&#39;output_file&#39;]),&#39;w&#39;) as f:
        f.write(&#39;id,scalar_coupling_constant\n&#39;)
        for i in range(idx.shape[0]):
            f.write(str(int(idx[i]))+&#39;,&#39;+str(x_out[i])+&#39;\n&#39;)


if __name__==&#39;__main__&#39;:
    batch_size = 64
    if &#39;fast&#39; not in sys.argv:
        print(&#34;Loading submission loaders...&#34;)
        with gzip.open(os.path.join(root,settings[&#39;PROCESSED_DATA_DIR&#39;],&#39;torch_proc_submission.pkl.gz&#39;),&#39;rb&#39;) as f:
            sub_dataset = TensorDataset(*pickle.load(f))
        loader = DataLoader(sub_dataset, batch_size=batch_size, shuffle=False, num_workers=4)
        for modelname in models[&#39;names&#39;]:
            model = load_model(modelname)
            if model is None:
                continue
            print(&#39;Predicting {}...&#39;.format(modelname))
            single_model_predict(loader, model, modelname)
            # Free up memory
            del model
    idx, x_out = ensemble(models[&#39;names&#39;])
    write_final(idx,x_out)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="autoENRICH.ml.models.BCAI.predictor.ensemble"><code class="name flex">
<span>def <span class="ident">ensemble</span></span>(<span>modelnames)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ensemble(modelnames):
    xs = [load_submission(name) for name in modelnames]
    n_model = len(xs)
    idx = xs[0][:,0]
    x_all = np.vstack(( [xs[i][:,1] for i in range(n_model)] ))

    with open(os.path.join(root,settings[&#39;RAW_DATA_DIR&#39;],&#39;test.csv&#39;),&#39;r&#39;) as f:
        lines = f.read().strip().split(&#39;\n&#39;)[1:]
        types = [&#39;1JHC&#39;,&#39;1JHN&#39;,&#39;2JHC&#39;,&#39;2JHH&#39;,&#39;2JHN&#39;,&#39;3JHC&#39;,&#39;3JHH&#39;,&#39;3JHN&#39;]
        line_types = np.array([types.index(line.split(&#39;,&#39;)[4]) for line in lines])
        line_type_indices = [(line_types == i) for i in range(8)]

    median_index = (n_model-1)/2
    even_models = False
    if n_model % 2 == 0:
        even_models = True
        print(&#39;WARNING: even number of models supplied&#39;)
    median_index = int(median_index)
    indices = np.argsort(x_all, axis=0)
    chosen_models = np.zeros((n_model,8), dtype=np.int)
    for i in range(n_model):
        for j in range(8):
            chosen_models[i,j] = int((indices[median_index, line_type_indices[j]] == i).sum())
            if even_models:
                chosen_models[i,j] += int((indices[median_index+1, line_type_indices[j]] == i).sum())
    if even_models:
        chosen_models = chosen_models / 2.0
    print(&#39;Count of a model &amp; type being chosen in a raw median procedure:&#39;)
    for i in range(n_model):
        print(str(chosen_models[i,:])+&#39; &#39;+modelnames[i])
    print(&#39;&#39;)

    model_mask = select_models(n_model,chosen_models)
    print(&#39;Model mask based on model count for median (now hardcoded):&#39;)
    for i in range(n_model):
        print(str(model_mask[i,:])+&#39; &#39;+modelnames[i])
    print(&#39;&#39;)

    x_out = np.zeros(x_all.shape[1])
    answer_count = 0
    for i in range(8):
        sorted_x_part = np.sort(x_all[ model_mask[:,i],: ][ :,line_type_indices[i] ], axis=0)
        model_count = model_mask[:,i].sum()
        start_idx = int((model_count - median_mean_counts[i])/2)
        end_idx = start_idx + median_mean_counts[i]
        mean_x_median = np.mean(sorted_x_part[ start_idx:end_idx,: ], axis=0)
        x_out[ line_type_indices[i] ] = mean_x_median
        print(&#39;Type &#39;+str(i)+&#39;: &#39;+str(model_count)+&#39; models average &#39;+str(median_mean_counts[i])+&#39; median&#39;)
        print(&#39;Output shape after mean: &#39;,str(mean_x_median.shape))
        print(&#39;Sorted indices for median mean: &#39;,str([j for j in range(start_idx,end_idx)]))
        answer_count += mean_x_median.shape[0]
    assert answer_count == 2505542
    return idx,x_out</code></pre>
</details>
</dd>
<dt id="autoENRICH.ml.models.BCAI.predictor.load_model"><code class="name flex">
<span>def <span class="ident">load_model</span></span>(<span>name)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model(name):
    model_folder = os.path.join(root,settings[&#39;MODEL_DIR&#39;],models[name+&#39;_dir&#39;])
    if not os.path.isdir(model_folder):
        sys.stderr.write(&#34;Error reading model from {}\n&#34;.format(model_folder))
        return None
    if not os.path.isfile(os.path.join(model_folder,&#39;model.ckpt&#39;)):
        sys.stderr.write(&#34;Error reading model from {}/model.ckpt\n&#34;.format(model_folder))
        return None
    sys.path = [model_folder] + sys.path
    import graph_transformer
    importlib.reload(graph_transformer)
    print(&#34;Loading {} from {}&#34;.format(name,graph_transformer.__file__))
    with open(os.path.join(model_folder,&#39;config&#39;)) as f:
        # JSON standard is double quotes, but some config files use single quotes.
        config_str = f.read().replace(&#34;&#39;&#34;,&#39;&#34;&#39;)
        config = json.loads(config_str)
    # Clean it up if necessary
    to_del = [&#39;name&#39;,&#39;optim&#39;,&#39;lr&#39;,&#39;mom&#39;,&#39;scheduler&#39;,&#39;warmup_step&#39;,&#39;decay_rate&#39;,
            &#39;lr_min&#39;,&#39;clip&#39;,&#39;max_epoch&#39;,&#39;batch_size&#39;,&#39;seed&#39;,&#39;cuda&#39;,&#39;debug&#39;,
            &#39;patience&#39;,&#39;champs_loss&#39;,&#39;multi_gpu&#39;,&#39;fp16&#39;,&#39;max_bond_count&#39;,
            &#39;log_interval&#39;,&#39;batch_chunk&#39;,&#39;work_dir&#39;,&#39;restart&#39;,&#39;restart_dir&#39;,
            &#39;load&#39;,&#39;mode&#39;,&#39;eta_min&#39;,&#39;gpu0_bsz&#39;,&#39;n_all_param&#39;,&#39;max_step&#39;,&#39;d_embed&#39;,
            &#39;cutout&#39;]
    for new,old in {&#39;dim&#39;:&#39;d_model&#39;, &#39;n_layers&#39;:&#39;n_layer&#39;, &#39;fdim&#39;:&#39;feature_dim&#39;,
            &#39;dist_embedding&#39;:&#39;dist_embed_type&#39;, &#39;atom_angle_embedding&#39;:&#39;angle_embed_type&#39;,
            &#39;trip_angle_embedding&#39;:&#39;quad_angle_embed_type&#39;,
            &#39;quad_angle_embedding&#39;:&#39;quad_angle_embed_type&#39;,
            }.items():
        if old in config and new not in config:
            config[new] = config[old]
            to_del.append(old)
    for old in to_del:
        if old in config:
            del config[old]
    # It would be nice to read the atom types from loaders, but it takes too long.
    config.update({k:models[k] for k in models if k.startswith(&#39;num&#39;) and k.endswith(&#39;types&#39;)})
    model = graph_transformer.GraphTransformer(**config)
    to_load_st = torch.load(os.path.join(model_folder,&#39;model.ckpt&#39;)).state_dict()
    model.load_state_dict(to_load_st)
    sys.path.remove(model_folder)
    return model</code></pre>
</details>
</dd>
<dt id="autoENRICH.ml.models.BCAI.predictor.load_submission"><code class="name flex">
<span>def <span class="ident">load_submission</span></span>(<span>modelname)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_submission(modelname):
    data = pd.read_csv(os.path.join(root,settings[&#39;SUBMISSION_DIR&#39;],modelname+&#39;.csv.bz2&#39;))
    sort_idx = np.argsort(data[&#39;id&#39;])
    out = np.vstack((data[&#39;id&#39;],data[&#39;scalar_coupling_constant&#39;]))[:,sort_idx].T
    return out</code></pre>
</details>
</dd>
<dt id="autoENRICH.ml.models.BCAI.predictor.select_models"><code class="name flex">
<span>def <span class="ident">select_models</span></span>(<span>n_model, chosen_models, hardcoded=True)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_models(n_model,chosen_models,hardcoded=True):
    if not hardcoded:
        model_mask = np.zeros((n_model,8), dtype=np.bool)
        for i in range(8):
            best_models = np.flip( np.argsort( chosen_models[:,i] ) )[0:model_count_for_median[i]]
            for j in best_models:
                model_mask[j,i] = True
    else:
        #assert models[&#39;names&#39;] == [&#34;gt15_3047fine3069&#34;, &#34;gt16_3044fine3068&#34;, &#34;gt18_3020&#34;, &#34;gtA_174932_3015fine3042&#34;, &#34;gtB_124323_2997&#34;, &#34;gtC_091310_3010fine3025&#34;, &#34;gtD_092424_3018fine3038&#34;, &#34;gtE_114919_2823&#34;, &#34;gtF_115725_2940&#34;, &#34;gtG_120830_3001&#34;, &#34;gtH_125215_3020&#34;, &#34;gt15_J_3049&#34;, &#34;gt14_K_3074&#34;]
        model_mask = [[ True,  True,  True,  True,  True,  True,  True,  True],  # gt15_3047fine3069
                      [ True,  True,  True,  True,  True,  True,  True,  True],  # gt16_3044fine3068
                      [ True, False,  True, False, False,  True,  True, False],  # gt18_3020
                      [ True,  True,  True,  True,  True,  True,  True,  True],  # gtA_174932_3015fine3042
                      [False,  True, False,  True,  True, False,  True,  True],  # gtB_124323_2997
                      [ True,  True,  True,  True,  True, False, False,  True],  # gtC_091310_3010fine3025
                      [ True,  True,  True,  True,  True,  True,  True,  True],  # gtD_092424_3018fine3038
                      [False, False, False, False,  True, False, False, False],  # gtE_114919_2823
                      [False, False, False,  True,  True, False, False,  True],  # gtF_115725_2940
                      [False, False, False, False, False,  True, False, False],  # gtG_120830_3001
                      [ True,  True,  True, False, False,  True,  True, False],  # gtH_125215_3020
                      [ True,  True,  True,  True, False,  True,  True,  True],  # gt15_J_3049
                      [ True,  True,  True,  True,  True,  True,  True,  True]]  # gt14_K_3074
        model_mask = np.array(model_mask, dtype=np.bool)
    return model_mask</code></pre>
</details>
</dd>
<dt id="autoENRICH.ml.models.BCAI.predictor.single_model_predict"><code class="name flex">
<span>def <span class="ident">single_model_predict</span></span>(<span>loader, model, modelname)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def single_model_predict(loader, model, modelname):
    MAX_BOND_COUNT = 406
    out_str = &#34;id,scalar_coupling_constant\n&#34;
    dev = &#34;cuda&#34;
    #dev = &#34;cpu&#34;
    model = model.to(dev)
    model.eval()
    with torch.no_grad():
        for arr in tqdm(loader):
            x_idx, x_atom, x_atom_pos, x_bond, x_bond_dist, x_triplet, x_triplet_angle, x_quad, x_quad_angle, y = arr
            x_atom, x_atom_pos, x_bond, x_bond_dist, x_triplet, x_triplet_angle, x_quad, x_quad_angle, y = \
                x_atom.to(dev), x_atom_pos.to(dev), x_bond.to(dev), x_bond_dist.to(dev), \
                x_triplet.to(dev), x_triplet_angle.to(dev), x_quad.to(dev), x_quad_angle.to(dev), y.to(dev)

            x_bond, x_bond_dist, y = x_bond[:, :MAX_BOND_COUNT], x_bond_dist[:, :MAX_BOND_COUNT], y[:,:MAX_BOND_COUNT]
            y_pred, _ = model(x_atom, x_atom_pos, x_bond, x_bond_dist, x_triplet, x_triplet_angle, x_quad, x_quad_angle)
            y_pred_pad = torch.cat([torch.zeros(y_pred.shape[0], 1, y_pred.shape[2], device=y_pred.device), y_pred], dim=1)
            y_pred_scaled = y_pred_pad.gather(1,x_bond[:,:,1][:,None,:])[:,0,:] * y[:,:,2] + y[:,:,1]

            y_selected = y_pred_scaled.masked_select((x_bond[:,:,0] &gt; 0) &amp; (y[:,:,3] &gt; 0)).cpu().numpy()
            ids_selected = y[:,:,0].masked_select((x_bond[:,:,0] &gt; 0) &amp; (y[:,:,3] &gt; 0))

            if dev==&#39;cuda&#39;:
                ids_selected = ids_selected.cpu()
            ids_selected = ids_selected.numpy()

            for id_, pred in zip(ids_selected, y_selected):
                out_str += &#34;{0:d},{1:f}\n&#34;.format(int(id_), pred)
    with open(os.path.join(root,settings[&#39;SUBMISSION_DIR&#39;],modelname+&#39;.csv.bz2&#39;), &#34;wb&#34;) as f:
        f.write(bz2.compress(out_str.encode(&#39;utf-8&#39;)))
    return</code></pre>
</details>
</dd>
<dt id="autoENRICH.ml.models.BCAI.predictor.write_final"><code class="name flex">
<span>def <span class="ident">write_final</span></span>(<span>idx, x_out)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_final(idx,x_out):
    with open(os.path.join(root,settings[&#39;SUBMISSION_DIR&#39;],models[&#39;output_file&#39;]),&#39;w&#39;) as f:
        f.write(&#39;id,scalar_coupling_constant\n&#39;)
        for i in range(idx.shape[0]):
            f.write(str(int(idx[i]))+&#39;,&#39;+str(x_out[i])+&#39;\n&#39;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="autoENRICH.ml.models.BCAI" href="index.html">autoENRICH.ml.models.BCAI</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="autoENRICH.ml.models.BCAI.predictor.ensemble" href="#autoENRICH.ml.models.BCAI.predictor.ensemble">ensemble</a></code></li>
<li><code><a title="autoENRICH.ml.models.BCAI.predictor.load_model" href="#autoENRICH.ml.models.BCAI.predictor.load_model">load_model</a></code></li>
<li><code><a title="autoENRICH.ml.models.BCAI.predictor.load_submission" href="#autoENRICH.ml.models.BCAI.predictor.load_submission">load_submission</a></code></li>
<li><code><a title="autoENRICH.ml.models.BCAI.predictor.select_models" href="#autoENRICH.ml.models.BCAI.predictor.select_models">select_models</a></code></li>
<li><code><a title="autoENRICH.ml.models.BCAI.predictor.single_model_predict" href="#autoENRICH.ml.models.BCAI.predictor.single_model_predict">single_model_predict</a></code></li>
<li><code><a title="autoENRICH.ml.models.BCAI.predictor.write_final" href="#autoENRICH.ml.models.BCAI.predictor.write_final">write_final</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>